{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc7b64a-207a-4608-90bd-b5b20fe693b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389b327-18c3-408b-84fc-59698bbc503b",
   "metadata": {},
   "source": [
    "# 1. Transformer Architecture by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fb5be-b4c1-43ef-9c77-c955090a5fee",
   "metadata": {},
   "source": [
    "### Embeddings of the words\n",
    "1) Words need to be turned into numerical vectors so their meaning can be understood by the model.\n",
    "2) We also need to include positional encoding to keep track of the order and context of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b2440c-7e5e-4c4b-afcd-3b839578c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32f63dc-4439-4ce8-a3a9-7ec0ecca1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476fba3-22c1-4d11-b213-8246a5082d1d",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "Applying normalization over a single sample, across all embeddings. The purpose is to achieve a mean equal to 0 and unit variance so the model converges faster and the vanishing/exploding gradient problem becomes less likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90312b24-616d-475c-bc62-f71ce7b1482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345876c-9499-42a1-9c65-c1281a597a08",
   "metadata": {},
   "source": [
    "### FeedForward Block\n",
    "Each of the layers in encoder and decoder contains a fully connected feed-forward network, which is applied to each input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91d844c-6cfc-4915-9272-f44701997724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70140f-3164-4d31-8bbf-cd76963c2edd",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Block\n",
    "Full implementation of it. This class will be used for both self-attention (in encoder) and cross-attention (decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7fcf24-4c97-43e4-b87d-f79aad00c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        # obtaining the keys, queries and values matrices from the embeddings\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        # getting the size of the last dimension (d_k)\n",
    "        d_k = query.size(-1)\n",
    "\n",
    "        # formula of attention\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # making the values for the words which are not seen yet very low so after applying softmax it becomes 0 (needed in decoder part)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        \n",
    "        return torch.matmul(attention_probs, value)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # split the matrix tensor into h heads and changing dimensions\n",
    "        def transform(x):\n",
    "            return x.view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        query = transform(self.w_q(q))\n",
    "        key = transform(self.w_k(k))\n",
    "        value = transform(self.w_v(v))\n",
    "        \n",
    "        # calculation of the final matrix with attention values\n",
    "        attention_output = self.attention(query, key, value, mask)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.w_o(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569981a-2a8f-4c5b-aee6-c6ce783d5f9e",
   "metadata": {},
   "source": [
    "### Implementing Residual Connection\n",
    "Adding the input back to the output, with some regularization and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5964ba-93d6-468d-974d-83c326b8afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db50fd-c71b-4643-87bd-bf956b429a53",
   "metadata": {},
   "source": [
    "###  Implementing Encoder\n",
    "At first defining an encoder block consisting of self-attention, feed-forward, and residual connections and then definind the whole the overall encoder composed of several such blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c9ae4f-55ed-4499-bc87-ade8850a0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    # One block of the encoder with attention and feed-forward layers.\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block  # Attention part\n",
    "        self.feed_forward_block = feed_forward_block  # Feed-forward part\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])  # Shortcut connections\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Apply attention layer with shortcut\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        # Apply feed-forward layer with shortcut\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # Full encoder made of multiple blocks.\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers  # All encoder blocks stacked together\n",
    "        self.norm = LayerNormalization(features)  # Normalizing output\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Pass through all blocks one by one\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        # Normalize final output\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9c886-1277-4b1c-9324-11537be3bafe",
   "metadata": {},
   "source": [
    "### Implementing Decoder\n",
    "At first defining a decoder block consisting of self-attention, cross-attention, feed-forward, and residual connections and then definind the whole  decoder composed of several such blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e993bca-9e95-4216-8763-f3cfc06a4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    # One block of the decoder with self-attention, cross-attention, and feed-forward.\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block  # Decoder's self-attention\n",
    "        self.cross_attention_block = cross_attention_block  # Attention over encoder output\n",
    "        self.feed_forward_block = feed_forward_block  # Feed-forward part\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])  # Three shortcuts\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        # Decoder looks at itself first\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        # Then it looks at encoder's output (cross-attention)\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        # Finally, applies feed-forward layer\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Full decoder made of multiple decoder blocks.\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers  # All decoder blocks stacked together\n",
    "        self.norm = LayerNormalization(features)  # Normalizing output\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        # Pass through all blocks one by one\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        # Normalize final output\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ebaf3-8306-46d8-9996-e6dcbf0c899b",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccda49d-50ae-4e7d-a9a0-05fc7278c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351114cc-6844-4c5c-b51c-db6f44163195",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "Implementation of the whole transformer architecture utilizing all the previosuly defined classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01eeb724-4d34-4459-9c41-101ef0fc1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, linear_layer: LinearLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.linear_layer = linear_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a115197-a44e-4c53-bdfa-3c8dd2da7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the linear layer\n",
    "    linear_layer = LinearLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos,linear_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b847d9c-02f7-4ec4-8594-b94bdb5bef7f",
   "metadata": {},
   "source": [
    "# 2. Training the Model\n",
    "Using https://huggingface.co/docs/transformers/en/tasks/translation dataset for training the tranformer model for MT (en to fr) task. </br>\n",
    "To prepare the data we need to take a dataset and a tokenizer, convert sentences into lists of numbers, make all sentences the same length by adding padding, and return the processed data as PyTorch tensors ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac91c210-15ca-4ab4-ae5b-85b3d3ed2991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520f38833f81446792cd012ea15fbf49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d212d360578428cbd2200f3d9b00c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42d67112bef403ebb5b614a3a24c2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "books = load_dataset(\"opus_books\", \"en-fr\")\n",
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f137592f-d01b-4690-8b1c-c731fdb5b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to their ID's\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts, max_vocab_size=20000):\n",
    "        self.word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<unk>\"}\n",
    "\n",
    "        word_counts = Counter(\" \".join(texts).split())\n",
    "        most_common = word_counts.most_common(max_vocab_size - len(self.word2idx))\n",
    "\n",
    "        for idx, (word, _) in enumerate(most_common, len(self.word2idx)):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.word2idx.get(word, self.word2idx[\"<unk>\"]) for word in text.split()]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return \" \".join([self.idx2word.get(idx, \"<unk>\") for idx in indices])\n",
    "\n",
    "\n",
    "def prepare_data(dataset, tokenizer, max_len):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for item in dataset:\n",
    "        en_text = item['translation']['en']\n",
    "        fr_text = item['translation']['fr']\n",
    "\n",
    "        en_tokens = tokenizer.encode(en_text)[:max_len]\n",
    "        fr_tokens = tokenizer.encode(fr_text)[:max_len]\n",
    "\n",
    "        en_tokens = en_tokens + [0] * (max_len - len(en_tokens))\n",
    "        fr_tokens = fr_tokens + [0] * (max_len - len(fr_tokens))\n",
    "\n",
    "        inputs.append(en_tokens)\n",
    "        targets.append(fr_tokens)\n",
    "\n",
    "    return torch.tensor(inputs), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3a50dfab-e779-487a-9fe9-1743c317265e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "\n",
    "train_subset = books['train'].select(range(5000))\n",
    "test_subset = books['test'].select(range(1000))\n",
    "\n",
    "train_texts = [item['translation']['en'] for item in train_subset] + [item['translation']['fr'] for item in train_subset]\n",
    "test_texts = [item['translation']['en'] for item in test_subset] + [item['translation']['fr'] for item in test_subset]\n",
    "\n",
    "simple_tokenizer = SimpleTokenizer(train_texts)\n",
    "\n",
    "# building the vocabulary\n",
    "train_inputs, train_targets = prepare_data(train_subset, simple_tokenizer, max_len)\n",
    "test_inputs, test_targets = prepare_data(test_subset, simple_tokenizer, max_len)\n",
    "\n",
    "# tokenizing and padding\n",
    "train_dataset = torch.utils.data.TensorDataset(train_inputs, train_targets)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_inputs, test_targets)\n",
    "\n",
    "# creating Datasets and DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, num_workers=0, pin_memory=True)\n",
    "\n",
    "# setting model hyperparameters\n",
    "src_vocab_size = simple_tokenizer.vocab_size\n",
    "tgt_vocab_size = simple_tokenizer.vocab_size\n",
    "src_seq_len = max_len\n",
    "tgt_seq_len = max_len\n",
    "\n",
    "# building the model\n",
    "transformer = build_transformer(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "transformer.to(device)\n",
    "\n",
    "# setting loss and optim\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3518e077-747c-4eac-af6f-b447d646e12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 8.4263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 6.9458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 6.5374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 6.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 6.2382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        src, tgt = batch\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        src_mask = torch.ones((src.shape[0], 1, 1, src.shape[1]), device=device)\n",
    "        tgt_mask = torch.ones((tgt_input.shape[0], 1, 1, tgt_input.shape[1]), device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_output = transformer.encode(src, src_mask)\n",
    "        decoder_output = transformer.decode(encoder_output, src_mask, tgt_input, tgt_mask)\n",
    "        output = transformer.project(decoder_output)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(transformer, train_dataloader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
