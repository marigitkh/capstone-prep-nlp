{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68fcb4a-f648-4d5d-87da-3dbc94515e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/marina/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "\n",
    "corpus = []\n",
    "for cat in ['news']:\n",
    "    for text_id in brown.fileids(cat):\n",
    "        raw_text = list(itertools.chain.from_iterable(brown.sents(text_id)))\n",
    "        text = ' '.join(raw_text)\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub('[^a-z ]+', '', text)\n",
    "        corpus.append([w for w in text.split() if w != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88a27b4-8363-416d-9650-8f3355100111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference in the amount of words after subsampling is 19137\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random, math\n",
    "\n",
    "def subsample_frequent_words(corpus):\n",
    "    filtered_corpus = []\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    sum_word_counts = sum(word_counts.values())\n",
    "    word_probs = {word: count / float(sum_word_counts) for word, count in word_counts.items()}\n",
    "    for text in corpus:\n",
    "        filtered_text = []\n",
    "        for word in text:\n",
    "            if random.random() < (1 + math.sqrt(word_probs[word] * 1e3)) * 1e-3 / word_probs[word]:\n",
    "                filtered_text.append(word)\n",
    "        filtered_corpus.append(filtered_text)\n",
    "    return filtered_corpus\n",
    "original = len(list(itertools.chain.from_iterable(corpus)))\n",
    "corpus = subsample_frequent_words(corpus)\n",
    "subsampled = len(list(itertools.chain.from_iterable(corpus)))\n",
    "vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "word_to_index = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "index_to_word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "print(f'The difference in the amount of words after subsampling is {original - subsampled}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ceff1c2-6517-490d-959e-0d8c50468f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multinomial\n",
    "\n",
    "def sample_negative(sample_size):\n",
    "    word_counts = dict(Counter(list(itertools.chain.from_iterable(corpus))))\n",
    "    normalizing_factor = sum([v**0.75 for v in word_counts.values()])\n",
    "    sample_probability = {word: (count**0.75) / normalizing_factor for word, count in word_counts.items()}\n",
    "    words = np.array(list(word_counts.keys()))\n",
    "    while True:\n",
    "        word_list = []\n",
    "        sampled_index = np.array(multinomial(sample_size, list(sample_probability.values())))\n",
    "        for index, count in enumerate(sampled_index):\n",
    "            for _ in range(count):\n",
    "                word_list.append(words[index])\n",
    "        yield word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08dcf513-7fae-41d7-b68c-5d70427f5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run it faster\n",
    "corpus = corpus[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4218c3c5-9c3c-46c8-902c-0ec0c7d1273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 246632 (target, context, negative) pairs.\n"
     ]
    }
   ],
   "source": [
    "context_tuple_list = []\n",
    "window_size = 4\n",
    "negative_samples = sample_negative(8)\n",
    "\n",
    "for text in corpus:\n",
    "    for i, word in enumerate(text):\n",
    "        first_context_index = max(0, i - window_size)\n",
    "        last_context_index = min(i + window_size + 1, len(text))\n",
    "        for j in range(first_context_index, last_context_index):\n",
    "            if i != j:\n",
    "                # Each tuple: (target, context, negative_samples)\n",
    "                context_tuple_list.append((word, text[j], next(negative_samples)))\n",
    "print(\"There are {} (target, context, negative) pairs.\".format(len(context_tuple_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e093aa40-a50b-40ed-a838-332bff957004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings_target = nn.Embedding(vocab_size, embedding_size)  # (vocab_size, embedding_size)\n",
    "        self.embeddings_context = nn.Embedding(vocab_size, embedding_size)  # (vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, target_word, context_word, negative_example):\n",
    "        # target_word: (1,) \n",
    "        # context_word: (1,)\n",
    "        # negative_example: (1, num_negative_samples(8))\n",
    "        emb_target = self.embeddings_target(target_word)  # (1, embedding_size)\n",
    "        emb_context = self.embeddings_context(context_word)  # (1, embedding_size)\n",
    "        \n",
    "        positive_dot = torch.mul(emb_context, emb_target)  # (1, embedding_size)\n",
    "        positive_dot = torch.sum(positive_dot, dim=1)  # (1,)\n",
    "        loss = torch.sum(F.logsigmoid(positive_dot))  # scalar\n",
    "        \n",
    "        emb_negative = self.embeddings_context(negative_example)  # (1, num_negative_samples(8), embedding_size)\n",
    "        negative_dot = torch.bmm(emb_negative, emb_target.unsqueeze(2))  # (1, num_negative_samples, 1)\n",
    "        negative_dot = torch.sum(negative_dot, dim=1).squeeze()  # (1,)\n",
    "        \n",
    "        loss += torch.sum(F.logsigmoid(-negative_dot))  # scalar\n",
    "\n",
    "        return -loss  # scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f9b363-c0a0-40e1-acf0-fe3a26547171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_percent_gain=0.5):\n",
    "        self.patience = patience\n",
    "        self.loss_list = []\n",
    "        self.min_percent_gain = min_percent_gain / 100.\n",
    "        \n",
    "    def update_loss(self, loss):\n",
    "        self.loss_list.append(loss)\n",
    "        if len(self.loss_list) > self.patience:\n",
    "            del self.loss_list[0]\n",
    "    \n",
    "    def stop_training(self):\n",
    "        if len(self.loss_list) == 1:\n",
    "            return False\n",
    "        gain = (max(self.loss_list) - min(self.loss_list)) / max(self.loss_list)\n",
    "        print(\"Loss gain: {}%\".format(round(100 * gain, 2)))\n",
    "        return gain < self.min_percent_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d2c7d9-c2fc-460e-bc3c-d7eb212b17a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.autograd as autograd\n",
    "\n",
    "def get_batches(context_tuple_list, batch_size=100):\n",
    "    random.shuffle(context_tuple_list)\n",
    "    batches = []\n",
    "    batch_target, batch_context, batch_negative = [], [], []\n",
    "    for i in range(len(context_tuple_list)):\n",
    "        target_word, context_word, negative_words = context_tuple_list[i]\n",
    "        batch_target.append(word_to_index[target_word])\n",
    "        batch_context.append(word_to_index[context_word])\n",
    "        # convert list of negative words to their indices\n",
    "        batch_negative.append([word_to_index[w] for w in negative_words])\n",
    "        if (i + 1) % batch_size == 0 or i == len(context_tuple_list) - 1:\n",
    "            tensor_target = autograd.Variable(torch.from_numpy(np.array(batch_target)).long())\n",
    "            tensor_context = autograd.Variable(torch.from_numpy(np.array(batch_context)).long())\n",
    "            tensor_negative = autograd.Variable(torch.from_numpy(np.array(batch_negative)).long())\n",
    "            batches.append((tensor_target, tensor_context, tensor_negative))\n",
    "            batch_target, batch_context, batch_negative = [], [], []\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c10781c-78c0-40e8-9f71-863521250c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 42397.76349861391\n",
      "Loss: 38790.30517578125\n",
      "Loss gain: 8.51%\n",
      "Loss: 35493.140829763106\n",
      "Loss gain: 16.29%\n",
      "Loss: 32383.90882528982\n",
      "Loss gain: 23.62%\n",
      "Loss: 29459.450848979333\n",
      "Loss gain: 30.52%\n",
      "Loss: 26726.051332535284\n",
      "Loss gain: 31.1%\n",
      "Loss: 24193.03875535534\n",
      "Loss gain: 31.84%\n",
      "Loss: 21871.57712481099\n",
      "Loss gain: 32.46%\n",
      "Loss: 19766.390569871473\n",
      "Loss gain: 32.9%\n",
      "Loss: 17872.488990045364\n",
      "Loss gain: 33.13%\n",
      "Loss: 16179.082220262097\n",
      "Loss gain: 33.13%\n",
      "Loss: 14671.6027359501\n",
      "Loss gain: 32.92%\n",
      "Loss: 13330.2945280998\n",
      "Loss gain: 32.56%\n",
      "Loss: 12137.003374653477\n",
      "Loss gain: 32.09%\n",
      "Loss: 11073.591493668095\n",
      "Loss gain: 31.56%\n",
      "Loss: 10124.873763545867\n",
      "Loss gain: 30.99%\n",
      "Loss: 9276.044289865802\n",
      "Loss gain: 30.41%\n",
      "Loss: 8513.596624558972\n",
      "Loss gain: 29.85%\n",
      "Loss: 7826.966893349924\n",
      "Loss gain: 29.32%\n",
      "Loss: 7206.751321115801\n",
      "Loss gain: 28.82%\n",
      "Loss: 6645.241756316154\n",
      "Loss gain: 28.36%\n",
      "Loss: 6136.243212299963\n",
      "Loss gain: 27.92%\n",
      "Loss: 5674.303422497165\n",
      "Loss gain: 27.5%\n",
      "Loss: 5253.9009999921245\n",
      "Loss gain: 27.1%\n",
      "Loss: 4869.345346758442\n",
      "Loss gain: 26.72%\n",
      "Loss: 4516.974118140436\n",
      "Loss gain: 26.39%\n",
      "Loss: 4193.741446218183\n",
      "Loss gain: 26.09%\n",
      "Loss: 3896.1751640073717\n",
      "Loss gain: 25.84%\n",
      "Loss: 3620.663231634325\n",
      "Loss gain: 25.64%\n",
      "Loss: 3365.3821770452682\n",
      "Loss gain: 25.49%\n",
      "Loss: 3128.6864619101248\n",
      "Loss gain: 25.4%\n",
      "Loss: 2909.102830456149\n",
      "Loss gain: 25.33%\n",
      "Loss: 2705.3041120959865\n",
      "Loss gain: 25.28%\n",
      "Loss: 2515.3985699069117\n",
      "Loss gain: 25.26%\n",
      "Loss: 2338.526384907384\n",
      "Loss gain: 25.26%\n",
      "Loss: 2173.7217791157386\n",
      "Loss gain: 25.28%\n",
      "Loss: 2020.0948570005355\n",
      "Loss gain: 25.33%\n",
      "Loss: 1876.712619904549\n",
      "Loss gain: 25.39%\n",
      "Loss: 1742.4911415346207\n",
      "Loss gain: 25.49%\n",
      "Loss: 1617.3313559255291\n",
      "Loss gain: 25.6%\n",
      "Loss: 1500.277439978815\n",
      "Loss gain: 25.73%\n",
      "Loss: 1391.1600799560547\n",
      "Loss gain: 25.87%\n",
      "Loss: 1289.1619986257244\n",
      "Loss gain: 26.02%\n",
      "Loss: 1193.8731290755734\n",
      "Loss gain: 26.18%\n",
      "Loss: 1104.8973930112777\n",
      "Loss gain: 26.35%\n",
      "Loss: 1022.0754857217112\n",
      "Loss gain: 26.53%\n",
      "Loss: 944.7473402946226\n",
      "Loss gain: 26.72%\n",
      "Loss: 872.6825384324596\n",
      "Loss gain: 26.9%\n",
      "Loss: 805.6475700870637\n",
      "Loss gain: 27.08%\n",
      "Loss: 743.2640051072643\n",
      "Loss gain: 27.28%\n",
      "Loss: 685.4737366707094\n",
      "Loss gain: 27.44%\n",
      "Loss: 631.610209557318\n",
      "Loss gain: 27.62%\n",
      "Loss: 581.5451658925703\n",
      "Loss gain: 27.82%\n",
      "Loss: 535.2272045996881\n",
      "Loss gain: 27.99%\n",
      "Loss: 492.3008002004316\n",
      "Loss gain: 28.18%\n",
      "Loss: 452.4598223778509\n",
      "Loss gain: 28.36%\n",
      "Loss: 415.7203826904297\n",
      "Loss gain: 28.51%\n",
      "Loss: 381.7073226436492\n",
      "Loss gain: 28.68%\n",
      "Loss: 350.2620882218884\n",
      "Loss gain: 28.85%\n",
      "Loss: 321.3100951410109\n",
      "Loss gain: 28.99%\n",
      "Loss: 294.69303574100616\n",
      "Loss gain: 29.11%\n",
      "Loss: 270.0724110141877\n",
      "Loss gain: 29.25%\n",
      "Loss: 247.6804671133718\n",
      "Loss gain: 29.29%\n",
      "Loss: 227.03411453000962\n",
      "Loss gain: 29.34%\n",
      "Loss: 208.24805924200243\n",
      "Loss gain: 29.33%\n",
      "Loss: 191.08604289639382\n",
      "Loss gain: 29.25%\n",
      "Loss: 175.3995684654482\n",
      "Loss gain: 29.18%\n",
      "Loss: 161.18281795132546\n",
      "Loss gain: 29.01%\n",
      "Loss: 148.06143437662433\n",
      "Loss gain: 28.9%\n",
      "Loss: 136.16204464820123\n",
      "Loss gain: 28.74%\n",
      "Loss: 125.25915035124748\n",
      "Loss gain: 28.59%\n",
      "Loss: 115.2422322611655\n",
      "Loss gain: 28.5%\n",
      "Loss: 106.17536194093766\n",
      "Loss gain: 28.29%\n",
      "Loss: 97.85828545785719\n",
      "Loss gain: 28.13%\n",
      "Loss: 90.34111032178325\n",
      "Loss gain: 27.88%\n",
      "Loss: 83.40645424012214\n",
      "Loss gain: 27.63%\n",
      "Loss: 77.14923563311177\n",
      "Loss gain: 27.34%\n",
      "Loss: 71.3788572742093\n",
      "Loss gain: 27.06%\n",
      "Loss: 66.20082642955165\n",
      "Loss gain: 26.72%\n",
      "Loss: 61.35099804785944\n",
      "Loss gain: 26.44%\n",
      "Loss: 56.935275370074855\n",
      "Loss gain: 26.2%\n",
      "Loss: 52.90806633426297\n",
      "Loss gain: 25.88%\n",
      "Loss: 49.229377192835656\n",
      "Loss gain: 25.64%\n",
      "Loss: 45.81418620386431\n",
      "Loss gain: 25.32%\n",
      "Loss: 42.734816597354026\n",
      "Loss gain: 24.94%\n",
      "Loss: 39.88121637221305\n",
      "Loss gain: 24.62%\n",
      "Loss: 37.16731508316532\n",
      "Loss gain: 24.5%\n",
      "Loss: 34.81896332771547\n",
      "Loss gain: 24.0%\n",
      "Loss: 32.528534662339\n",
      "Loss gain: 23.88%\n",
      "Loss: 30.51641587288149\n",
      "Loss gain: 23.48%\n",
      "Loss: 28.62736988067627\n",
      "Loss gain: 22.98%\n",
      "Loss: 26.870603669074274\n",
      "Loss gain: 22.83%\n",
      "Loss: 25.188860554848947\n",
      "Loss gain: 22.56%\n",
      "Loss: 23.717049871721574\n",
      "Loss gain: 22.28%\n",
      "Loss: 22.333738034771336\n",
      "Loss gain: 21.98%\n",
      "Loss: 21.0736033262745\n",
      "Loss gain: 21.57%\n",
      "Loss: 19.81978907892781\n",
      "Loss gain: 21.32%\n",
      "Loss: 18.734602743579494\n",
      "Loss gain: 21.01%\n",
      "Loss: 17.650810703154534\n",
      "Loss gain: 20.97%\n",
      "Loss: 16.69948302545855\n",
      "Loss gain: 20.76%\n",
      "Loss: 15.868851561700144\n",
      "Loss gain: 19.93%\n",
      "Loss: 14.9453512237918\n",
      "Loss gain: 20.23%\n",
      "Loss: 14.246519396381993\n",
      "Loss gain: 19.29%\n",
      "Loss: 13.44518736100966\n",
      "Loss gain: 19.49%\n",
      "Loss: 12.756691682723261\n",
      "Loss gain: 19.61%\n",
      "Loss: 12.126327599248578\n",
      "Loss gain: 18.86%\n",
      "Loss: 11.565043987766389\n",
      "Loss gain: 18.82%\n",
      "Loss: 10.935942634459465\n",
      "Loss gain: 18.66%\n",
      "Loss: 10.452618723915469\n",
      "Loss gain: 18.06%\n",
      "Loss: 9.943088869894705\n",
      "Loss gain: 18.0%\n",
      "Loss: 9.539683795744374\n",
      "Loss gain: 17.51%\n",
      "Loss: 9.04671698616397\n",
      "Loss gain: 17.28%\n",
      "Loss: 8.728842552631132\n",
      "Loss gain: 16.49%\n",
      "Loss: 8.279204524332478\n",
      "Loss gain: 16.73%\n",
      "Loss: 8.005447294442884\n",
      "Loss gain: 16.08%\n",
      "Loss: 7.661195670404742\n",
      "Loss gain: 15.32%\n",
      "Loss: 7.303391948823006\n",
      "Loss gain: 16.33%\n",
      "Loss: 6.973304462048315\n",
      "Loss gain: 15.77%\n",
      "Loss: 6.691772571494503\n",
      "Loss gain: 16.41%\n",
      "Loss: 6.541868842417194\n",
      "Loss gain: 14.61%\n",
      "Loss: 6.186155999860456\n",
      "Loss gain: 15.3%\n",
      "Loss: 5.996725224679516\n",
      "Loss gain: 14.0%\n",
      "Loss: 5.77236996927569\n",
      "Loss gain: 13.74%\n",
      "Loss: 5.621830257677263\n",
      "Loss gain: 14.06%\n",
      "Loss: 5.364100288479559\n",
      "Loss gain: 13.29%\n",
      "Loss: 5.286483331072715\n",
      "Loss gain: 11.84%\n",
      "Loss: 5.001984272753039\n",
      "Loss gain: 13.35%\n",
      "Loss: 4.9221581658048015\n",
      "Loss gain: 12.45%\n",
      "Loss: 4.696962016244089\n",
      "Loss gain: 12.44%\n",
      "Loss: 4.657398942978151\n",
      "Loss gain: 11.9%\n",
      "Loss: 4.545721309800302\n",
      "Loss gain: 9.12%\n",
      "Loss: 4.246364504579575\n",
      "Loss gain: 13.73%\n",
      "Loss: 4.324604760735266\n",
      "Loss gain: 9.59%\n",
      "Loss: 4.1924630259313895\n",
      "Loss gain: 9.98%\n",
      "Loss: 4.058627909229648\n",
      "Loss gain: 10.72%\n",
      "Loss: 3.8938874025498666\n",
      "Loss gain: 9.96%\n",
      "Loss: 3.832358081494608\n",
      "Loss gain: 11.38%\n",
      "Loss: 3.7519949496753755\n",
      "Loss gain: 10.51%\n",
      "Loss: 3.7395171479832743\n",
      "Loss gain: 7.86%\n",
      "Loss: 3.6747693779968444\n",
      "Loss gain: 5.63%\n",
      "Loss: 3.5588012626094203\n",
      "Loss gain: 7.14%\n",
      "Loss: 3.450733639540211\n",
      "Loss gain: 8.03%\n",
      "Loss: 3.382365986945168\n",
      "Loss gain: 9.55%\n",
      "Loss: 3.300643951662125\n",
      "Loss gain: 10.18%\n",
      "Loss: 3.283958758077314\n",
      "Loss gain: 7.72%\n",
      "Loss: 3.209751834311793\n",
      "Loss gain: 6.98%\n",
      "Loss: 3.1070918959956013\n",
      "Loss gain: 8.14%\n",
      "Loss: 3.121871101760095\n",
      "Loss gain: 5.86%\n",
      "Loss: 3.121012570877229\n",
      "Loss gain: 5.39%\n",
      "Loss: 3.0477330747150604\n",
      "Loss gain: 5.05%\n",
      "Loss: 3.012018333519659\n",
      "Loss gain: 3.52%\n",
      "Loss: 2.9697895504293905\n",
      "Loss gain: 4.87%\n",
      "Loss: 2.9047495775645777\n",
      "Loss gain: 6.93%\n",
      "Loss: 2.9066310520133665\n",
      "Loss gain: 4.69%\n",
      "Loss: 2.8645222876821794\n",
      "Loss gain: 4.9%\n",
      "Loss: 2.91765423095034\n",
      "Loss gain: 3.54%\n",
      "Loss: 2.874600671472088\n",
      "Loss gain: 1.82%\n",
      "Loss: 2.7898711449196263\n",
      "Loss gain: 4.38%\n",
      "Loss: 2.650259299143668\n",
      "Loss gain: 9.16%\n",
      "Loss: 2.775500016827737\n",
      "Loss gain: 9.16%\n",
      "Loss: 2.6638606265187263\n",
      "Loss gain: 7.8%\n",
      "Loss: 2.740213908435356\n",
      "Loss gain: 5.0%\n",
      "Loss: 2.667214559931909\n",
      "Loss gain: 4.51%\n",
      "Loss: 2.6387960192176605\n",
      "Loss gain: 4.93%\n",
      "Loss: 2.635302851517354\n",
      "Loss gain: 3.83%\n",
      "Loss: 2.4891706657025123\n",
      "Loss gain: 9.16%\n",
      "Loss: 2.629194819999318\n",
      "Loss gain: 6.68%\n",
      "Loss: 2.608045584732486\n",
      "Loss gain: 5.67%\n",
      "Loss: 2.5445693865899117\n",
      "Loss gain: 5.55%\n",
      "Loss: 2.53707449955325\n",
      "Loss gain: 5.33%\n",
      "Loss: 2.563640869072368\n",
      "Loss gain: 3.5%\n",
      "Loss: 2.581641984202208\n",
      "Loss gain: 2.72%\n",
      "Loss: 2.520230878024332\n",
      "Loss gain: 2.38%\n",
      "Loss: 2.5212217036995197\n",
      "Loss gain: 2.38%\n",
      "Loss: 2.4330244054717403\n",
      "Loss gain: 5.76%\n",
      "Loss: 2.48685085953724\n",
      "Loss gain: 5.76%\n",
      "Loss: 2.5200673603723125\n",
      "Loss gain: 3.5%\n",
      "Loss: 2.5036968705394576\n",
      "Loss gain: 3.5%\n",
      "Loss: 2.493866330313106\n",
      "Loss gain: 3.45%\n",
      "Loss: 2.4427783053007817\n",
      "Loss gain: 3.07%\n",
      "Loss: 2.471913637112706\n",
      "Loss gain: 3.07%\n",
      "Loss: 2.373083416793135\n",
      "Loss gain: 5.22%\n",
      "Loss: 2.43226748194185\n",
      "Loss gain: 4.84%\n",
      "Loss: 2.462298575668566\n",
      "Loss gain: 4.0%\n",
      "Loss: 2.4599667103780853\n",
      "Loss gain: 4.0%\n",
      "Loss: 2.3217688003374684\n",
      "Loss gain: 5.71%\n",
      "Loss: 2.4654217913987173\n",
      "Loss gain: 5.83%\n",
      "Loss: 2.3913287411353763\n",
      "Loss gain: 5.83%\n",
      "Loss: 2.422954176417402\n",
      "Loss gain: 5.83%\n",
      "Loss: 2.434600125157064\n",
      "Loss gain: 5.83%\n",
      "Loss: 2.4648013084886538\n",
      "Loss gain: 3.01%\n",
      "Loss: 2.4293279446541303\n",
      "Loss gain: 2.98%\n",
      "Loss: 2.3106639389128936\n",
      "Loss gain: 6.25%\n",
      "Loss: 2.480438383296132\n",
      "Loss gain: 6.84%\n",
      "Loss: 2.3435231021515306\n",
      "Loss gain: 6.84%\n",
      "Loss: 2.43166461415709\n",
      "Loss gain: 6.84%\n",
      "Loss: 2.3061729003224642\n",
      "Loss gain: 7.03%\n",
      "Loss: 2.36126072332263\n",
      "Loss gain: 7.03%\n",
      "Loss: 2.3959871433194606\n",
      "Loss gain: 5.16%\n",
      "Loss: 2.2871938158697898\n",
      "Loss gain: 5.94%\n",
      "Loss: 2.443859007690222\n",
      "Loss gain: 6.41%\n",
      "Loss: 2.320925231091678\n",
      "Loss gain: 6.41%\n",
      "Loss: 2.3385366520453847\n",
      "Loss gain: 6.41%\n",
      "Loss: 2.3638197194724793\n",
      "Loss gain: 6.41%\n",
      "Loss: 2.312925134515089\n",
      "Loss gain: 5.36%\n",
      "Loss: 2.3370326496059857\n",
      "Loss gain: 2.15%\n",
      "Loss: 2.2644499469428294\n",
      "Loss gain: 4.2%\n",
      "Loss: 2.445924327228098\n",
      "Loss gain: 7.42%\n",
      "Loss: 2.3949789156115826\n",
      "Loss gain: 7.42%\n",
      "Loss: 2.2615896077646362\n",
      "Loss gain: 7.54%\n",
      "Loss: 2.368833945718624\n",
      "Loss gain: 7.54%\n",
      "Loss: 2.35865089256737\n",
      "Loss gain: 7.54%\n",
      "Loss: 2.302367775438113\n",
      "Loss gain: 5.57%\n",
      "Loss: 2.294805338726409\n",
      "Loss gain: 4.53%\n",
      "Loss: 2.3934101499767313\n",
      "Loss gain: 4.12%\n",
      "Loss: 2.331806000292061\n",
      "Loss gain: 4.12%\n",
      "Loss: 2.2952228004803823\n",
      "Loss gain: 4.12%\n",
      "Loss: 2.428265245811593\n",
      "Loss gain: 5.5%\n",
      "Loss: 2.2535062985374563\n",
      "Loss gain: 7.2%\n",
      "Loss: 2.3537558046199623\n",
      "Loss gain: 7.2%\n",
      "Loss: 2.314383898294651\n",
      "Loss gain: 7.2%\n",
      "Loss: 2.385629373001716\n",
      "Loss gain: 7.2%\n",
      "Loss: 2.273382090768146\n",
      "Loss gain: 5.54%\n",
      "Loss: 2.3500394123876767\n",
      "Loss gain: 4.71%\n",
      "Loss: 2.244329977592814\n",
      "Loss gain: 5.92%\n",
      "Loss: 2.2970451223513773\n",
      "Loss gain: 5.92%\n",
      "Loss: 2.2883342452001787\n",
      "Loss gain: 4.5%\n",
      "Loss: 2.322217283500058\n",
      "Loss gain: 4.5%\n",
      "Loss: 2.290894983929672\n",
      "Loss gain: 3.35%\n",
      "Loss: 2.358129597779724\n",
      "Loss gain: 2.96%\n",
      "Loss: 2.277399407339192\n",
      "Loss gain: 3.42%\n",
      "Loss: 2.304578988061797\n",
      "Loss gain: 3.42%\n",
      "Loss: 2.2675891283884524\n",
      "Loss gain: 3.84%\n",
      "Loss: 2.3023513069042125\n",
      "Loss gain: 3.84%\n",
      "Loss: 2.3560290371927795\n",
      "Loss gain: 3.75%\n",
      "Loss: 2.303846847930653\n",
      "Loss gain: 3.75%\n",
      "Loss: 2.281923541132616\n",
      "Loss gain: 3.75%\n",
      "Loss: 2.377192565528188\n",
      "Loss gain: 4.01%\n",
      "Loss: 2.2655100272893303\n",
      "Loss gain: 4.7%\n",
      "Loss: 2.2647250981085123\n",
      "Loss gain: 4.73%\n",
      "Loss: 2.2984691432511974\n",
      "Loss gain: 4.73%\n",
      "Loss: 2.3208228253248717\n",
      "Loss gain: 4.73%\n",
      "Loss: 2.210178104343809\n",
      "Loss gain: 4.77%\n",
      "Loss: 2.3269700061045437\n",
      "Loss gain: 5.02%\n",
      "Loss: 2.2787268239133542\n",
      "Loss gain: 5.02%\n",
      "Loss: 2.3436225500678822\n",
      "Loss gain: 5.69%\n",
      "Loss: 2.1732981942252314\n",
      "Loss gain: 7.27%\n",
      "Loss: 2.309127432496978\n",
      "Loss gain: 7.27%\n",
      "Loss: 2.3270466704176918\n",
      "Loss gain: 7.27%\n",
      "Loss: 2.2495806913094354\n",
      "Loss gain: 7.27%\n",
      "Loss: 2.3753138441543635\n",
      "Loss gain: 8.5%\n",
      "Loss: 2.2694187829000576\n",
      "Loss gain: 5.29%\n",
      "Loss: 2.237814400638003\n",
      "Loss gain: 5.79%\n",
      "Loss: 2.3033989072766063\n",
      "Loss gain: 5.79%\n",
      "Loss: 2.251937617249261\n",
      "Loss gain: 5.79%\n",
      "Loss: 2.2671410959033715\n",
      "Loss gain: 2.85%\n",
      "Loss: 2.3044794088740264\n",
      "Loss gain: 2.89%\n",
      "Loss: 2.257499984491827\n",
      "Loss gain: 2.28%\n",
      "Loss: 2.34283645181436\n",
      "Loss gain: 3.88%\n",
      "Loss: 2.2550609262074315\n",
      "Loss gain: 3.75%\n",
      "Loss: 2.2756642170171553\n",
      "Loss gain: 3.75%\n",
      "Loss: 2.2448991338812534\n",
      "Loss gain: 4.18%\n",
      "Loss: 2.354718267557121\n",
      "Loss gain: 4.66%\n",
      "Loss: 2.3011377028512556\n",
      "Loss gain: 4.66%\n",
      "Loss: 2.2109762453680646\n",
      "Loss gain: 6.1%\n",
      "Loss: 2.338561848496958\n",
      "Loss gain: 6.1%\n",
      "Loss: 2.2644833976519294\n",
      "Loss gain: 6.1%\n",
      "Loss: 2.3279141756525683\n",
      "Loss gain: 5.46%\n",
      "Loss: 2.2284567809847724\n",
      "Loss gain: 5.46%\n",
      "Loss: 2.3699404608585963\n",
      "Loss gain: 5.97%\n",
      "Loss: 2.2608413330782504\n",
      "Loss gain: 5.97%\n",
      "Loss: 2.2350078424130895\n",
      "Loss gain: 5.97%\n",
      "Loss: 2.308928750558651\n",
      "Loss gain: 5.97%\n",
      "Loss: 2.3111668836471866\n",
      "Loss gain: 5.69%\n",
      "Loss: 2.215600297642654\n",
      "Loss gain: 4.13%\n",
      "Loss: 2.3389988786751226\n",
      "Loss gain: 5.28%\n",
      "Loss: 2.2711773253019687\n",
      "Loss gain: 5.28%\n",
      "Loss: 2.2690566758918127\n",
      "Loss gain: 5.28%\n",
      "Loss: 2.281671804283002\n",
      "Loss gain: 5.28%\n",
      "Loss: 2.3136244588606663\n",
      "Loss gain: 2.99%\n",
      "Loss: 2.160700898054206\n",
      "Loss gain: 6.61%\n",
      "Loss: 2.254157373415036\n",
      "Loss gain: 6.61%\n",
      "Loss: 2.313747385794188\n",
      "Loss gain: 6.61%\n",
      "Loss: 2.206025383247423\n",
      "Loss gain: 6.61%\n",
      "Loss: 2.284165057806193\n",
      "Loss gain: 6.61%\n",
      "Loss: 2.257019358430801\n",
      "Loss gain: 4.66%\n",
      "Loss: 2.2147726757344395\n",
      "Loss gain: 4.66%\n",
      "Loss: 2.322346419073461\n",
      "Loss gain: 5.01%\n",
      "Loss: 2.278663478404445\n",
      "Loss gain: 4.63%\n",
      "Loss: 2.259030728231958\n",
      "Loss gain: 4.63%\n",
      "Loss: 2.3247687005491424\n",
      "Loss gain: 4.73%\n",
      "Loss: 2.3140368025933933\n",
      "Loss gain: 2.83%\n",
      "Loss: 2.307955999364851\n",
      "Loss gain: 2.83%\n",
      "Loss: 2.260636751021065\n",
      "Loss gain: 2.83%\n",
      "Loss: 2.244709629932561\n",
      "Loss gain: 3.44%\n",
      "Loss: 2.2305843568033343\n",
      "Loss gain: 3.61%\n",
      "Loss: 2.2785500647019474\n",
      "Loss gain: 3.35%\n",
      "Loss: 2.3469244102616944\n",
      "Loss gain: 4.96%\n",
      "Loss: 2.2380090071162537\n",
      "Loss gain: 4.96%\n",
      "Loss: 2.2510774740045645\n",
      "Loss gain: 4.96%\n",
      "Loss: 2.238525582402344\n",
      "Loss gain: 4.64%\n",
      "Loss: 2.2767168434452727\n",
      "Loss gain: 4.64%\n",
      "Loss: 2.316082651351368\n",
      "Loss gain: 3.37%\n",
      "Loss: 2.289021013704534\n",
      "Loss gain: 3.35%\n",
      "Loss: 2.2235742431717704\n",
      "Loss gain: 3.99%\n",
      "Loss: 2.302533915272497\n",
      "Loss gain: 3.99%\n",
      "Loss: 2.192689333065984\n",
      "Loss gain: 5.33%\n",
      "Loss: 2.2958887105661203\n",
      "Loss gain: 4.77%\n",
      "Loss: 2.3114391430281103\n",
      "Loss gain: 5.14%\n",
      "Loss: 2.2947189812248032\n",
      "Loss gain: 5.14%\n",
      "Loss: 2.2516164768509745\n",
      "Loss gain: 5.14%\n",
      "Loss: 2.2312747651022002\n",
      "Loss gain: 3.47%\n",
      "Loss: 2.2465417879434573\n",
      "Loss gain: 3.47%\n",
      "Loss: 2.150028532547895\n",
      "Loss gain: 6.31%\n",
      "Loss: 2.313493115311083\n",
      "Loss gain: 7.07%\n",
      "Loss: 2.216950680399614\n",
      "Loss gain: 7.07%\n",
      "Loss: 2.2228806562545227\n",
      "Loss gain: 7.07%\n",
      "Loss: 2.2857659631502636\n",
      "Loss gain: 7.07%\n",
      "Loss: 2.298245179023774\n",
      "Loss gain: 4.17%\n",
      "Loss: 2.2127613001437334\n",
      "Loss gain: 3.72%\n",
      "Loss: 2.2252281109498244\n",
      "Loss gain: 3.72%\n",
      "Loss: 2.241891755233331\n",
      "Loss gain: 3.72%\n",
      "Loss: 2.2207089852534176\n",
      "Loss gain: 3.72%\n",
      "Loss: 2.2705573559473007\n",
      "Loss gain: 2.55%\n",
      "Loss: 2.349194602560132\n",
      "Loss gain: 5.47%\n",
      "Loss: 2.2706047283539985\n",
      "Loss gain: 5.47%\n",
      "Loss: 2.2800541173186972\n",
      "Loss gain: 5.47%\n",
      "Loss: 2.190846256475197\n",
      "Loss gain: 6.74%\n",
      "Loss: 2.269612500774524\n",
      "Loss gain: 6.74%\n",
      "Loss: 2.279201197916747\n",
      "Loss gain: 3.91%\n",
      "Loss: 2.2123210461337997\n",
      "Loss gain: 3.91%\n",
      "Loss: 2.317390213971873\n",
      "Loss gain: 5.46%\n",
      "Loss: 2.2252473101402903\n",
      "Loss gain: 4.53%\n",
      "Loss: 2.3412535204786824\n",
      "Loss gain: 5.51%\n",
      "Loss: 2.2201410393876535\n",
      "Loss gain: 5.51%\n",
      "Loss: 2.233620359892807\n",
      "Loss gain: 5.17%\n",
      "Loss: 2.2873306143878103\n",
      "Loss gain: 5.17%\n",
      "Loss: 2.2250262885654886\n",
      "Loss gain: 5.17%\n",
      "Loss: 2.2362606175499216\n",
      "Loss gain: 2.94%\n",
      "Loss: 2.271487152048071\n",
      "Loss gain: 2.72%\n",
      "Loss: 2.2825558111174713\n",
      "Loss gain: 2.72%\n",
      "Loss: 2.2494215862543863\n",
      "Loss gain: 2.52%\n",
      "Loss: 2.274225180247618\n",
      "Loss gain: 2.03%\n",
      "Loss: 2.2350341359945767\n",
      "Loss gain: 2.08%\n",
      "Loss: 2.2657487612572544\n",
      "Loss gain: 2.08%\n",
      "Loss: 2.184638766046133\n",
      "Loss gain: 3.94%\n",
      "Loss: 2.2547810771427446\n",
      "Loss gain: 3.94%\n",
      "Loss: 2.1950291932171826\n",
      "Loss gain: 3.58%\n",
      "Loss: 2.246877503605755\n",
      "Loss gain: 3.58%\n",
      "Loss: 2.314866287393435\n",
      "Loss gain: 5.63%\n",
      "Loss: 2.2470799191524424\n",
      "Loss gain: 5.18%\n",
      "Loss: 2.2107358735426508\n",
      "Loss gain: 5.18%\n",
      "Loss: 2.2619325434169433\n",
      "Loss gain: 4.5%\n",
      "Loss: 2.228862636359296\n",
      "Loss gain: 4.5%\n",
      "Loss: 2.2200019252890226\n",
      "Loss gain: 2.26%\n",
      "Loss: 2.2470179552647975\n",
      "Loss gain: 2.26%\n",
      "Loss: 2.2253399223329557\n",
      "Loss gain: 1.85%\n",
      "Loss: 2.2439071164173523\n",
      "Loss gain: 1.2%\n",
      "Loss: 2.28172412041233\n",
      "Loss gain: 2.71%\n",
      "Loss: 2.199291343398316\n",
      "Loss gain: 3.61%\n",
      "Loss: 2.266036882884838\n",
      "Loss gain: 3.61%\n",
      "Loss: 2.304619406169701\n",
      "Loss gain: 4.57%\n",
      "Loss: 2.195784928941829\n",
      "Loss gain: 4.72%\n",
      "Loss: 2.215248758859572\n",
      "Loss gain: 4.72%\n",
      "Loss: 2.2945589869012757\n",
      "Loss gain: 4.72%\n",
      "Loss: 2.235650077864017\n",
      "Loss gain: 4.72%\n",
      "Loss: 2.2228498956875784\n",
      "Loss gain: 4.3%\n",
      "Loss: 2.2353561864630693\n",
      "Loss gain: 3.46%\n",
      "Loss: 2.1903617753578732\n",
      "Loss gain: 4.54%\n",
      "Loss: 2.2574626693902013\n",
      "Loss gain: 2.97%\n",
      "Loss: 2.250300950076405\n",
      "Loss gain: 2.97%\n",
      "Loss: 2.2515087077814724\n",
      "Loss gain: 2.97%\n",
      "Loss: 2.167349874252273\n",
      "Loss gain: 3.99%\n",
      "Loss: 2.2847449359213634\n",
      "Loss gain: 5.14%\n",
      "Loss: 2.270887675498201\n",
      "Loss gain: 5.14%\n",
      "Loss: 2.2375554467021908\n",
      "Loss gain: 5.14%\n",
      "Loss: 2.229563908855198\n",
      "Loss gain: 5.14%\n",
      "Loss: 2.2147260971739136\n",
      "Loss gain: 3.06%\n",
      "Loss: 2.246821544116228\n",
      "Loss gain: 2.47%\n",
      "Loss: 2.195677908254795\n",
      "Loss gain: 2.28%\n",
      "Loss: 2.2509963308369945\n",
      "Loss gain: 2.46%\n",
      "Loss: 2.2326673342547787\n",
      "Loss gain: 2.46%\n",
      "Loss: 2.174743408570066\n",
      "Loss gain: 3.39%\n",
      "Loss: 2.182624755214126\n",
      "Loss gain: 3.39%\n",
      "Loss: 2.2794815115029774\n",
      "Loss gain: 4.59%\n",
      "Loss: 2.2025495292355997\n",
      "Loss gain: 4.59%\n",
      "Loss: 2.261167401523468\n",
      "Loss gain: 4.59%\n",
      "Loss: 2.194717393303196\n",
      "Loss gain: 4.25%\n",
      "Loss: 2.2361119400255474\n",
      "Loss gain: 3.72%\n",
      "Loss: 2.209234888920741\n",
      "Loss gain: 2.94%\n",
      "Loss: 2.2726605297714064\n",
      "Loss gain: 3.43%\n",
      "Loss: 2.223202099027995\n",
      "Loss gain: 3.43%\n",
      "Loss: 2.2450791437501807\n",
      "Loss gain: 2.79%\n",
      "Loss: 2.16474181823639\n",
      "Loss gain: 4.75%\n",
      "Loss: 2.219424874453257\n",
      "Loss gain: 4.75%\n",
      "Loss: 2.2603483846722803\n",
      "Loss gain: 4.23%\n",
      "Loss: 2.259307329702067\n",
      "Loss gain: 4.23%\n",
      "Loss: 2.153887610385878\n",
      "Loss gain: 4.71%\n",
      "Loss: 2.2652454531374633\n",
      "Loss gain: 4.92%\n",
      "Loss: 2.1781971291995665\n",
      "Loss gain: 4.92%\n",
      "Loss: 2.286616979287036\n",
      "Loss gain: 5.8%\n",
      "Loss: 2.2348136975553876\n",
      "Loss gain: 5.8%\n",
      "Loss: 2.1797919716158387\n",
      "Loss gain: 4.74%\n",
      "Loss: 2.236064572995078\n",
      "Loss gain: 4.74%\n",
      "Loss: 2.2160039471101856\n",
      "Loss gain: 4.67%\n",
      "Loss: 2.260344225154709\n",
      "Loss gain: 3.56%\n",
      "Loss: 2.2001060557104797\n",
      "Loss gain: 3.56%\n",
      "Loss: 2.2827358156022046\n",
      "Loss gain: 3.62%\n",
      "Loss: 2.2075867546429375\n",
      "Loss gain: 3.62%\n",
      "Loss: 2.2356697953861926\n",
      "Loss gain: 3.62%\n",
      "Loss: 2.2226618288430826\n",
      "Loss gain: 3.62%\n",
      "Loss: 2.2212059654224094\n",
      "Loss gain: 3.29%\n",
      "Loss: 2.2046370746552824\n",
      "Loss gain: 1.39%\n",
      "Loss: 2.167825648125032\n",
      "Loss gain: 3.03%\n",
      "Loss: 2.197960025970749\n",
      "Loss gain: 2.47%\n",
      "Loss: 2.236333473184664\n",
      "Loss gain: 3.06%\n",
      "Loss: 2.328782297571325\n",
      "Loss gain: 6.91%\n",
      "Loss: 2.1790710157619024\n",
      "Loss gain: 6.91%\n",
      "Loss: 2.21178238952416\n",
      "Loss gain: 6.43%\n",
      "Loss: 2.1417351418753126\n",
      "Loss gain: 8.03%\n",
      "Loss: 2.252279851465456\n",
      "Loss gain: 8.03%\n",
      "Loss: 2.191379176441607\n",
      "Loss gain: 4.91%\n",
      "Loss: 2.212311486400957\n",
      "Loss gain: 4.91%\n",
      "Loss: 2.249436554923323\n",
      "Loss gain: 4.91%\n",
      "Loss: 2.2000300970699285\n",
      "Loss gain: 2.7%\n",
      "Loss: 2.189271877465334\n",
      "Loss gain: 2.67%\n",
      "Loss: 2.2401577056997697\n",
      "Loss gain: 2.67%\n",
      "Loss: 2.227192527713581\n",
      "Loss gain: 2.67%\n",
      "Loss: 2.1964976053397196\n",
      "Loss gain: 2.27%\n",
      "Loss: 2.1583620803308627\n",
      "Loss gain: 3.65%\n",
      "Loss: 2.258679966738958\n",
      "Loss gain: 4.44%\n",
      "Loss: 2.214844150172036\n",
      "Loss gain: 4.44%\n",
      "Loss: 2.2085927762354034\n",
      "Loss gain: 4.44%\n",
      "Loss: 2.2325164905689174\n",
      "Loss gain: 4.44%\n",
      "Loss: 2.214898776444193\n",
      "Loss gain: 2.22%\n",
      "Loss: 2.1669110621460863\n",
      "Loss gain: 2.94%\n",
      "Loss: 2.1987010297335443\n",
      "Loss gain: 2.94%\n",
      "Loss: 2.2616632872565288\n",
      "Loss gain: 4.19%\n",
      "Loss: 2.2503580374257437\n",
      "Loss gain: 4.19%\n",
      "Loss: 2.1836140750433954\n",
      "Loss gain: 4.19%\n",
      "Loss: 2.1530063894658062\n",
      "Loss gain: 4.8%\n",
      "Loss: 2.2001266628034384\n",
      "Loss gain: 4.8%\n",
      "Loss: 2.252013132777891\n",
      "Loss gain: 4.4%\n",
      "Loss: 2.188383823310423\n",
      "Loss gain: 4.4%\n",
      "Loss: 2.1504714121486272\n",
      "Loss gain: 4.51%\n",
      "Loss: 2.212921750106034\n",
      "Loss gain: 4.51%\n",
      "Loss: 2.1715965827876764\n",
      "Loss gain: 4.51%\n",
      "Loss: 2.228496700995639\n",
      "Loss gain: 3.5%\n",
      "Loss: 2.1663769177112338\n",
      "Loss gain: 3.5%\n",
      "Loss: 2.2632067366958513\n",
      "Loss gain: 4.28%\n",
      "Loss: 2.1995307275905245\n",
      "Loss gain: 4.28%\n",
      "Loss: 2.1981248282789885\n",
      "Loss gain: 4.28%\n",
      "Loss: 2.144405562787377\n",
      "Loss gain: 5.25%\n",
      "Loss: 2.202902061322088\n",
      "Loss gain: 5.25%\n",
      "Loss: 2.2097727913398204\n",
      "Loss gain: 2.96%\n",
      "Loss: 2.208819429696192\n",
      "Loss gain: 2.96%\n",
      "Loss: 2.198397589726336\n",
      "Loss gain: 2.96%\n",
      "Loss: 2.1870688492612493\n",
      "Loss gain: 1.03%\n",
      "Loss: 2.201587892786202\n",
      "Loss gain: 1.03%\n",
      "Loss: 2.194410542123801\n",
      "Loss gain: 0.98%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "net = Word2Vec(embedding_size=200, vocab_size=vocabulary_size)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "early_stopping = EarlyStopping(patience=5, min_percent_gain=1)\n",
    "\n",
    "while True:\n",
    "    losses = []\n",
    "    context_tuple_batches = get_batches(context_tuple_list, batch_size=2000)\n",
    "    for target_tensor, context_tensor, negative_tensor in context_tuple_batches:\n",
    "        net.zero_grad()\n",
    "        loss = net(target_tensor, context_tensor, negative_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    avg_loss = np.mean(losses)\n",
    "    print(\"Loss:\", avg_loss)\n",
    "    early_stopping.update_loss(avg_loss)\n",
    "    if early_stopping.stop_training():\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a78eb1-8a1e-4fd9-994c-31bb47a927c2",
   "metadata": {},
   "source": [
    "I decided to calculate words similariy using  the cosine similarity between two vectors because the method given in tutorial, for nearly any word returned 5 words like \"the\", \"for\", \"and\" etc, even though subsampling was applied. However, using cosine similarity still didn't give precise results, which I believe may be due to the small training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709d5dae-9b5d-4664-98b0-ef7886524d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_closest_word_cosine(word, topn=5):\n",
    "    word_distance = []\n",
    "    emb = net.embeddings_context\n",
    "    i = word_to_index[word]\n",
    "    lookup_tensor_i = torch.tensor([i], dtype=torch.long)\n",
    "    v_i = emb(lookup_tensor_i)\n",
    "    \n",
    "    for j in range(len(vocabulary)):\n",
    "        if j != i:\n",
    "            lookup_tensor_j = torch.tensor([j], dtype=torch.long)\n",
    "            v_j = emb(lookup_tensor_j)\n",
    "            cosine_sim = F.cosine_similarity(v_i, v_j, dim=1)\n",
    "            word_distance.append((index_to_word[j], float(cosine_sim)))\n",
    "    \n",
    "    word_distance.sort(key=lambda x: x[1], reverse=True)\n",
    "    return word_distance[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c777be3-b7a7-4e1f-8aa4-a6d803dabee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countin', 0.24084702134132385),\n",
       " ('remember', 0.23879709839820862),\n",
       " ('busied', 0.23643550276756287),\n",
       " ('ltd', 0.2311418205499649),\n",
       " ('purged', 0.23105531930923462)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_word_cosine(\"government\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c0a6d-befe-466b-a69a-64837f8b10ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
